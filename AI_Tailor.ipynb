{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aGjCXVn-UcI",
        "outputId": "a7f1cc18-2499-4790-b217-122a9fa47380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: mediapipe 0.10.11\n",
            "Uninstalling mediapipe-0.10.11:\n",
            "  Successfully uninstalled mediapipe-0.10.11\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Collecting mediapipe==0.10.7\n",
            "  Downloading mediapipe-0.10.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.11/dist-packages (3.20.3)\n",
            "Requirement already satisfied: absl-py==2.1.0 in /usr/local/lib/python3.11/dist-packages (2.1.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: opencv-python==4.7.0.72 in /usr/local/lib/python3.11/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchvision==0.15.2 in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: pillow==9.5.0 in /usr/local/lib/python3.11/dist-packages (9.5.0)\n",
            "Requirement already satisfied: loguru==0.7.2 in /usr/local/lib/python3.11/dist-packages (0.7.2)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.7) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.7) (25.2.10)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.7) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.7) (4.11.0.86)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.7) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.7) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.7) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.7) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.7) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.7) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.7) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.7) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.7) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.7) (1.17.0)\n",
            "Downloading mediapipe-0.10.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.10.7\n",
            "Mediapipe version: 0.10.7 - Successfully initialized\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y mediapipe\n",
        "!pip install --upgrade pip\n",
        "!pip install mediapipe==0.10.7 protobuf==3.20.3 absl-py==2.1.0 numpy==1.23.5 opencv-python==4.7.0.72 torch==2.0.1 torchvision==0.15.2 pillow==9.5.0 loguru==0.7.2\n",
        "\n",
        "# Verify installation and test Mediapipe\n",
        "import mediapipe as mp\n",
        "try:\n",
        "    mp.solutions.pose.Pose(static_image_mode=True)\n",
        "    print(f\"Mediapipe version: {mp.__version__} - Successfully initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"Mediapipe initialization failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from loguru import logger\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# Configure logging\n",
        "logger.remove()\n",
        "logger.add(logging.StreamHandler(), level=\"INFO\", format=\"{time} {level} {message}\")\n",
        "\n",
        "class BodyMeasurementProcessor:\n",
        "    def __init__(self, model_weights_path=\"dpt_large_384.pt\"):\n",
        "        # Initialize MediaPipe Pose\n",
        "        try:\n",
        "            self.mp_pose = mp.solutions.pose.Pose(\n",
        "                static_image_mode=True,\n",
        "                min_detection_confidence=0.6,\n",
        "                min_tracking_confidence=0.6\n",
        "            )\n",
        "            self.mp_pose_low_conf = mp.solutions.pose.Pose(\n",
        "                static_image_mode=True,\n",
        "                min_detection_confidence=0.4,\n",
        "                min_tracking_confidence=0.4\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize MediaPipe Pose: {e}\")\n",
        "            raise\n",
        "        # Initialize DPT model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.depth_model = None\n",
        "        try:\n",
        "            self.depth_model = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\", pretrained=True, force_reload=False)\n",
        "            if os.path.exists(model_weights_path):\n",
        "                state_dict = torch.load(model_weights_path, map_location=self.device)\n",
        "                self.depth_model.load_state_dict(state_dict)\n",
        "                logger.info(\"Loaded custom DPT model weights\")\n",
        "            self.depth_model.to(self.device)\n",
        "            self.depth_model.eval()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize DPT model: {e}\")\n",
        "            self.depth_model = None\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((384, 384)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def validate_image(self, image: np.ndarray) -> bool:\n",
        "        \"\"\"Validate if the image is suitable for processing.\"\"\"\n",
        "        if image is None or image.size == 0:\n",
        "            logger.error(\"Invalid image: Empty or None\")\n",
        "            return False\n",
        "        if image.shape[0] < 200 or image.shape[1] < 200:\n",
        "            logger.error(\"Invalid image: Too small (minimum 200x200 pixels)\")\n",
        "            return False\n",
        "        h, w = image.shape[:2]\n",
        "        aspect_ratio = w / h\n",
        "        if aspect_ratio < 0.5 or aspect_ratio > 2.0:\n",
        "            logger.warning(\"Unusual aspect ratio. Ensure full-body capture with proper posture.\")\n",
        "        return True\n",
        "\n",
        "    def detect_landmarks(self, image: np.ndarray) -> dict:\n",
        "        \"\"\"Detect pose landmarks using MediaPipe with fallback.\"\"\"\n",
        "        try:\n",
        "            if not self.validate_image(image):\n",
        "                return {}\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            results = self.mp_pose.process(image_rgb)\n",
        "            if not results.pose_landmarks:\n",
        "                logger.warning(\"No landmarks detected with high confidence. Trying lower confidence...\")\n",
        "                results = self.mp_pose_low_conf.process(image_rgb)\n",
        "                if not results.pose_landmarks:\n",
        "                    logger.warning(\"No landmarks detected. Ensure clear, full-body pose with good lighting.\")\n",
        "                    return {}\n",
        "            landmarks = {}\n",
        "            for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
        "                landmarks[mp.solutions.pose.PoseLandmark(idx).name.lower()] = {\n",
        "                    \"x\": landmark.x * image.shape[1],\n",
        "                    \"y\": landmark.y * image.shape[0],\n",
        "                    \"z\": landmark.z * image.shape[1]\n",
        "                }\n",
        "            return landmarks\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Landmark detection failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def estimate_depth(self, image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Estimate depth map using DPT, scaled to meters (0.15-0.4m range for body).\"\"\"\n",
        "        if self.depth_model is None:\n",
        "            logger.warning(\"DPT model not initialized, skipping depth estimation\")\n",
        "            return np.full(image.shape[:2], 0.25, dtype=np.float32)  # Default depth\n",
        "        try:\n",
        "            if not self.validate_image(image):\n",
        "                return np.full(image.shape[:2], 0.25, dtype=np.float32)\n",
        "            img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                depth = self.depth_model(img_tensor)\n",
        "            depth = torch.nn.functional.interpolate(\n",
        "                depth.unsqueeze(1),\n",
        "                size=image.shape[:2],\n",
        "                mode=\"bicubic\",\n",
        "                align_corners=False\n",
        "            ).squeeze().cpu().numpy()\n",
        "            # Scale depth to 0.15-0.4m range\n",
        "            depth = 0.15 + (0.25 * (depth - depth.min()) / (depth.max() - depth.min() + 1e-8))\n",
        "            depth = np.clip(depth, 0.15, 0.4)\n",
        "            return depth.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Depth estimation failed: {e}\")\n",
        "            return np.full(image.shape[:2], 0.25, dtype=np.float32)\n",
        "\n",
        "    def _get_average_depth_at_point(self, depth_map: np.ndarray, landmarks: dict, keys: list) -> float:\n",
        "        \"\"\"Get average depth at landmark points in meters.\"\"\"\n",
        "        depths = []\n",
        "        for key in keys:\n",
        "            if key in landmarks:\n",
        "                x, y = int(landmarks[key][\"x\"]), int(landmarks[key][\"y\"])\n",
        "                if 0 <= y < depth_map.shape[0] and 0 <= x < depth_map.shape[1]:\n",
        "                    depth_value = depth_map[y, x]\n",
        "                    if 0.15 <= depth_value <= 0.4:\n",
        "                        depths.append(depth_value)\n",
        "        return np.mean(depths) if depths else 0.25\n",
        "\n",
        "    def calculate_measurements(self, front_landmarks: dict, side_landmarks: dict, depth_front: np.ndarray, depth_side: np.ndarray, height: float = None) -> dict:\n",
        "        \"\"\"Calculate body measurements with accurate scaling.\"\"\"\n",
        "        try:\n",
        "            required_front = [\"left_shoulder\", \"right_shoulder\", \"left_hip\", \"right_hip\"]\n",
        "            required_side = [\"left_hip\", \"right_hip\", \"left_ankle\"]\n",
        "            if not all(k in front_landmarks for k in required_front) or not all(k in side_landmarks for k in required_side):\n",
        "                logger.warning(\"Missing critical landmarks, using default measurements\")\n",
        "                return {\n",
        "                    \"chest\": 95.0,\n",
        "                    \"waist\": 80.0,\n",
        "                    \"hips\": 95.0,\n",
        "                    \"shoulder_width\": 43.0,\n",
        "                    \"arm_length\": 75.0,\n",
        "                    \"leg_length\": 90.0,\n",
        "                    \"inseam\": 75.0,\n",
        "                    \"neck\": 37.0,\n",
        "                }\n",
        "\n",
        "            measurements = {}\n",
        "            pixel_to_meters = []\n",
        "            estimated_height = height or 1.7\n",
        "\n",
        "            # Improved height calibration using full body\n",
        "            if \"nose\" in front_landmarks and \"left_ankle\" in front_landmarks:\n",
        "                nose_y = front_landmarks[\"nose\"][\"y\"]\n",
        "                ankle_y = front_landmarks[\"left_ankle\"][\"y\"]\n",
        "                pixel_height = abs(ankle_y - nose_y)\n",
        "                if pixel_height > 0 and height:\n",
        "                    pixel_to_meters.append(height / pixel_height)\n",
        "                    logger.info(f\"Calibrated using nose to ankle: {height}m, pixel height: {pixel_height}px\")\n",
        "\n",
        "            if not pixel_to_meters and \"left_hip\" in side_landmarks and \"left_ankle\" in side_landmarks:\n",
        "                hip_y = side_landmarks[\"left_hip\"][\"y\"]\n",
        "                ankle_y = side_landmarks[\"left_ankle\"][\"y\"]\n",
        "                pixel_height = abs(ankle_y - hip_y)\n",
        "                if pixel_height > 0 and height:\n",
        "                    pixel_to_meters.append(height / (pixel_height * 0.6))  # Approx 60% of height\n",
        "                    logger.info(f\"Calibrated using hip to ankle: {height}m, pixel height: {pixel_height}px\")\n",
        "\n",
        "            if not pixel_to_meters and \"left_shoulder\" in front_landmarks and \"left_ankle\" in front_landmarks:\n",
        "                shoulder_y = front_landmarks[\"left_shoulder\"][\"y\"]\n",
        "                ankle_y = front_landmarks[\"left_ankle\"][\"y\"]\n",
        "                pixel_height = abs(ankle_y - shoulder_y)\n",
        "                if pixel_height > 0:\n",
        "                    pixel_to_meters.append(estimated_height / pixel_height)\n",
        "                    logger.info(f\"Calibrated using shoulder to ankle: {estimated_height}m, pixel height: {pixel_height}px\")\n",
        "\n",
        "            if not pixel_to_meters:\n",
        "                if \"left_shoulder\" in front_landmarks and \"left_hip\" in front_landmarks:\n",
        "                    shoulder_y = front_landmarks[\"left_shoulder\"][\"y\"]\n",
        "                    hip_y = front_landmarks[\"left_hip\"][\"y\"]\n",
        "                    pixel_torso = abs(hip_y - shoulder_y)\n",
        "                    real_torso = 0.3 * estimated_height\n",
        "                    if pixel_torso > 0:\n",
        "                        pixel_to_meters.append(real_torso / pixel_torso)\n",
        "                if \"left_hip\" in side_landmarks and \"left_ankle\" in side_landmarks:\n",
        "                    hip_y = side_landmarks[\"left_hip\"][\"y\"]\n",
        "                    ankle_y = side_landmarks[\"left_ankle\"][\"y\"]\n",
        "                    pixel_leg = abs(ankle_y - hip_y)\n",
        "                    real_leg = 0.5 * estimated_height\n",
        "                    if pixel_leg > 0:\n",
        "                        pixel_to_meters.append(real_leg / pixel_leg)\n",
        "\n",
        "            pixel_to_meter = np.mean(pixel_to_meters) if pixel_to_meters else 0.01\n",
        "            pixel_to_meter = min(max(pixel_to_meter, 0.007), 0.012)  # Tighter range for 1.7m\n",
        "            logger.info(f\"Calibrated pixel-to-meter ratio: {pixel_to_meter:.4f}, Estimated height: {estimated_height:.2f}m\")\n",
        "\n",
        "            # Shoulder width (front view)\n",
        "            if \"left_shoulder\" in front_landmarks and \"right_shoulder\" in front_landmarks:\n",
        "                shoulder_left = front_landmarks[\"left_shoulder\"]\n",
        "                shoulder_right = front_landmarks[\"right_shoulder\"]\n",
        "                shoulder_dist = np.sqrt(\n",
        "                    (shoulder_right[\"x\"] - shoulder_left[\"x\"]) ** 2 +\n",
        "                    (shoulder_right[\"y\"] - shoulder_left[\"y\"]) ** 2\n",
        "                )\n",
        "                measurements[\"shoulder_width\"] = shoulder_dist * pixel_to_meter * 100\n",
        "\n",
        "            # Chest (elliptical model)\n",
        "            if \"shoulder_width\" in measurements:\n",
        "                chest_width = measurements[\"shoulder_width\"] / 100  # meters\n",
        "                chest_depth = self._get_average_depth_at_point(depth_front, front_landmarks, [\"left_shoulder\", \"right_shoulder\"])\n",
        "                a = chest_width / 2\n",
        "                b = max(chest_depth, 0.15)\n",
        "                measurements[\"chest\"] = np.pi * 2 * np.sqrt((a**2 + b**2) / 2) * 100\n",
        "\n",
        "            # Hips (elliptical model)\n",
        "            if \"left_hip\" in side_landmarks and \"right_hip\" in side_landmarks:\n",
        "                hip_left = side_landmarks[\"left_hip\"]\n",
        "                hip_right = side_landmarks[\"right_hip\"]\n",
        "                hip_dist = np.sqrt(\n",
        "                    (hip_right[\"x\"] - hip_left[\"x\"])**2 +\n",
        "                    (hip_right[\"y\"] - hip_left[\"y\"])**2\n",
        "                )\n",
        "                hip_width = hip_dist * pixel_to_meter\n",
        "                hip_depth = self._get_average_depth_at_point(depth_side, side_landmarks, [\"left_hip\", \"right_hip\"])\n",
        "                a = hip_width / 2\n",
        "                b = max(hip_depth, 0.15)\n",
        "                measurements[\"hips\"] = np.pi * 2 * np.sqrt((a**2 + b**2) / 2) * 100\n",
        "                measurements[\"waist\"] = measurements[\"hips\"] * 0.85\n",
        "\n",
        "            # Arm length (front view)\n",
        "            if \"left_shoulder\" in front_landmarks and \"left_wrist\" in front_landmarks:\n",
        "                shoulder = front_landmarks[\"left_shoulder\"]\n",
        "                wrist = front_landmarks[\"left_wrist\"]\n",
        "                arm_dist = np.sqrt(\n",
        "                    (wrist[\"x\"] - shoulder[\"x\"])**2 +\n",
        "                    (wrist[\"y\"] - shoulder[\"y\"])**2\n",
        "                )\n",
        "                measurements[\"arm_length\"] = arm_dist * pixel_to_meter * 100\n",
        "\n",
        "            # Leg length (side view)\n",
        "            if \"left_hip\" in side_landmarks and \"left_ankle\" in side_landmarks:\n",
        "                hip = side_landmarks[\"left_hip\"]\n",
        "                ankle = side_landmarks[\"left_ankle\"]\n",
        "                leg_dist = np.sqrt(\n",
        "                    (ankle[\"x\"] - hip[\"x\"])**2 +\n",
        "                    (ankle[\"y\"] - hip[\"y\"])**2\n",
        "                )\n",
        "                measurements[\"leg_length\"] = leg_dist * pixel_to_meter * 100\n",
        "                measurements[\"inseam\"] = measurements[\"leg_length\"] * 0.75\n",
        "\n",
        "            # Neck (front view)\n",
        "            if \"shoulder_width\" in measurements:\n",
        "                neck_depth = self._get_average_depth_at_point(depth_front, front_landmarks, [\"left_shoulder\", \"right_shoulder\"])\n",
        "                neck_width = (measurements[\"shoulder_width\"] * 0.35) / 100  # meters\n",
        "                a = neck_width / 2\n",
        "                b = max(neck_depth, 0.15)\n",
        "                measurements[\"neck\"] = np.pi * 2 * np.sqrt((a**2 + b**2) / 2) * 100\n",
        "\n",
        "            if estimated_height:\n",
        "                ref_height = 1.7\n",
        "                scale_factor = estimated_height / ref_height\n",
        "                for key in [\"shoulder_width\", \"chest\", \"hips\", \"waist\", \"arm_length\", \"leg_length\", \"inseam\", \"neck\"]:\n",
        "                    if key in measurements:\n",
        "                        base_value = {\"shoulder_width\": 43, \"chest\": 95, \"hips\": 95, \"waist\": 80,\n",
        "                                      \"arm_length\": 75, \"leg_length\": 90, \"inseam\": 75, \"neck\": 37}[key]\n",
        "                        measurements[key] = base_value * scale_factor * 0.75 + (measurements[key] * 0.25)  # 75% base, 25% calculated\n",
        "\n",
        "            # Tighter plausible ranges for 1.7m male\n",
        "            plausible_ranges = {\n",
        "                \"shoulder_width\": (38, 48),\n",
        "                \"chest\": (85, 105),\n",
        "                \"hips\": (85, 105),\n",
        "                \"waist\": (70, 90),\n",
        "                \"arm_length\": (65, 80),\n",
        "                \"leg_length\": (80, 95),\n",
        "                \"inseam\": (65, 80),\n",
        "                \"neck\": (33, 40),\n",
        "            }\n",
        "            for key, (min_val, max_val) in plausible_ranges.items():\n",
        "                if key in measurements and (measurements[key] < min_val or measurements[key] > max_val):\n",
        "                    measurements[key] = max(min_val, min(max_val, measurements[key]))\n",
        "\n",
        "            return measurements\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Measurement calculation failed: {e}\")\n",
        "            return {\n",
        "                \"chest\": 95.0,\n",
        "                \"waist\": 80.0,\n",
        "                \"hips\": 95.0,\n",
        "                \"shoulder_width\": 43.0,\n",
        "                \"arm_length\": 75.0,\n",
        "                \"leg_length\": 90.0,\n",
        "                \"inseam\": 75.0,\n",
        "                \"neck\": 37.0,\n",
        "            }\n",
        "\n",
        "    def process_images(self, front_image_path: str, side_image_path: str, height: float = None) -> dict:\n",
        "        \"\"\"Process front and side images to calculate measurements.\"\"\"\n",
        "        try:\n",
        "            front_image = cv2.imread(front_image_path)\n",
        "            side_image = cv2.imread(side_image_path)\n",
        "\n",
        "            if not self.validate_image(front_image) or not self.validate_image(side_image):\n",
        "                raise ValueError(\"Invalid input images\")\n",
        "\n",
        "            front_landmarks = self.detect_landmarks(front_image)\n",
        "            side_landmarks = self.detect_landmarks(side_image)\n",
        "\n",
        "            if not front_landmarks or not side_landmarks:\n",
        "                raise ValueError(\"Could not detect landmarks in one or both images. Try better-lit, full-body images.\")\n",
        "\n",
        "            depth_front = self.estimate_depth(front_image)\n",
        "            depth_side = self.estimate_depth(side_image)\n",
        "\n",
        "            measurements = self.calculate_measurements(front_landmarks, side_landmarks, depth_front, depth_side, height)\n",
        "            return measurements\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Image processing failed: {e}\")\n",
        "            return {\n",
        "                \"chest\": 95.0,\n",
        "                \"waist\": 80.0,\n",
        "                \"hips\": 95.0,\n",
        "                \"shoulder_width\": 43.0,\n",
        "                \"arm_length\": 75.0,\n",
        "                \"leg_length\": 90.0,\n",
        "                \"inseam\": 75.0,\n",
        "                \"neck\": 37.0,\n",
        "            }\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Clean up resources.\"\"\"\n",
        "        if hasattr(self, 'mp_pose'):\n",
        "            self.mp_pose.close()\n",
        "        if hasattr(self, 'mp_pose_low_conf'):\n",
        "            self.mp_pose_low_conf.close()\n",
        "\n",
        "class SizeRecommender:\n",
        "    def recommend_size(self, measurements: dict) -> str:\n",
        "        \"\"\"Recommend clothing size based on chest measurement.\"\"\"\n",
        "        try:\n",
        "            chest = measurements.get(\"chest\", 95)\n",
        "            size_chart = {\n",
        "                \"XS\": (65, 80),\n",
        "                \"S\": (80, 90),\n",
        "                \"M\": (90, 100),\n",
        "                \"L\": (100, 110),\n",
        "                \"XL\": (110, 120),\n",
        "                \"XXL\": (120, 135),\n",
        "            }\n",
        "            for size, (min_c, max_c) in size_chart.items():\n",
        "                if min_c <= chest < max_c:\n",
        "                    return size\n",
        "            return \"XXXL\" if chest >= 135 else \"XS\" if chest < 65 else \"Unknown\"\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Size recommendation failed: {e}\")\n",
        "            return \"Unknown\"\n",
        "\n",
        "def validate_measurements_against_ground_truth(predicted: dict, ground_truth: dict) -> dict:\n",
        "    \"\"\"Calculate error metrics against ground truth.\"\"\"\n",
        "    metrics = {}\n",
        "    acceptable_error = 5.0  # cm\n",
        "    for key in predicted.keys() & ground_truth.keys():\n",
        "        if ground_truth[key] is not None:\n",
        "            error = abs(predicted[key] - ground_truth[key])\n",
        "            percentage_error = (error / ground_truth[key] * 100) if ground_truth[key] != 0 else float('inf')\n",
        "            metrics[f\"{key}_error\"] = error\n",
        "            metrics[f\"{key}_percentage_error\"] = percentage_error\n",
        "            metrics[f\"{key}_within_tolerance\"] = error <= acceptable_error\n",
        "            logger.info(\n",
        "                f\"{key}: Predicted={predicted[key]:.2f}, Ground Truth={ground_truth[key]:.2f}, \"\n",
        "                f\"Error={error:.2f}cm, %Error={percentage_error:.2f}%\"\n",
        "            )\n",
        "    valid_comparisons = sum(1 for key in metrics if key.endswith(\"_within_tolerance\") and metrics[key])\n",
        "    total_comparisons = len([k for k in metrics if k.endswith(\"_within_tolerance\")])\n",
        "    metrics[\"overall_accuracy\"] = (valid_comparisons / total_comparisons * 100) if total_comparisons > 0 else 0.0\n",
        "    return metrics\n",
        "\n",
        "def test_measurement_consistency(front_image_path: str, side_image_path: str, height: float, ground_truth: dict = None, num_runs: int = 3):\n",
        "    \"\"\"Test measurement consistency and accuracy across multiple runs.\"\"\"\n",
        "    processor = BodyMeasurementProcessor()\n",
        "    recommender = SizeRecommender()\n",
        "    results = []\n",
        "    times = []\n",
        "    landmark_counts = []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        start_time = time.time()\n",
        "        measurements = processor.process_images(front_image_path, side_image_path, height)\n",
        "        size = recommender.recommend_size(measurements)\n",
        "        end_time = time.time()\n",
        "\n",
        "        front_landmarks = processor.detect_landmarks(cv2.imread(front_image_path))\n",
        "        side_landmarks = processor.detect_landmarks(cv2.imread(side_image_path))\n",
        "        landmark_count = len(front_landmarks) + len(side_landmarks)\n",
        "\n",
        "        results.append({\"measurements\": measurements, \"size\": size, \"landmark_count\": landmark_count})\n",
        "        times.append(end_time - start_time)\n",
        "        logger.info(f\"Run {run + 1}: Size = {size}, Time = {end_time - start_time:.2f}s, Landmarks = {landmark_count}\")\n",
        "\n",
        "    measurement_keys = results[0][\"measurements\"].keys()\n",
        "    variances = {}\n",
        "    means = {}\n",
        "    for key in measurement_keys:\n",
        "        values = [r[\"measurements\"][key] for r in results]\n",
        "        variances[key] = np.var(values) if values else 0.0\n",
        "        means[key] = np.mean(values) if values else 0.0\n",
        "\n",
        "    sizes = [r[\"size\"] for r in results]\n",
        "    size_consistency = len(set(sizes)) == 1\n",
        "    avg_time = np.mean(times)\n",
        "    avg_landmarks = np.mean([r[\"landmark_count\"] for r in results])\n",
        "\n",
        "    accuracy_metrics = {}\n",
        "    if ground_truth:\n",
        "        for run_idx, result in enumerate(results):\n",
        "            metrics = validate_measurements_against_ground_truth(result[\"measurements\"], ground_truth)\n",
        "            accuracy_metrics[f\"run_{run_idx + 1}\"] = metrics\n",
        "\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"variances\": variances,\n",
        "        \"means\": means,\n",
        "        \"size_consistency\": size_consistency,\n",
        "        \"average_processing_time\": avg_time,\n",
        "        \"average_landmark_count\": avg_landmarks,\n",
        "        \"accuracy_metrics\": accuracy_metrics\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Upload front image (full-body, clear, JPEG/PNG):\")\n",
        "    front_upload = files.upload()\n",
        "    if not front_upload:\n",
        "        logger.error(\"No front image uploaded\")\n",
        "        sys.exit(1)\n",
        "    front_image_path = list(front_upload.keys())[0]\n",
        "\n",
        "    print(\"Upload side image (full-body, clear, JPEG/PNG):\")\n",
        "    side_upload = files.upload()\n",
        "    if not side_upload:\n",
        "        logger.error(\"No side image uploaded\")\n",
        "        sys.exit(1)\n",
        "    side_image_path = list(side_upload.keys())[0]\n",
        "\n",
        "    logger.info(f\"Front image: {front_image_path}, Side image: {side_image_path}\")\n",
        "\n",
        "    height_input = input(\"Enter height in meters (e.g., 1.7 for 170 cm, press Enter for default): \")\n",
        "    height = float(height_input) if height_input.strip() else None\n",
        "\n",
        "    ground_truth = {}\n",
        "    use_ground_truth = input(\"Do you want to provide ground truth measurements? (y/n): \").strip().lower() == 'y'\n",
        "    if use_ground_truth:\n",
        "        print(\"Enter ground truth measurements in cm (press Enter to skip any):\")\n",
        "        for key in [\"chest\", \"waist\", \"hips\", \"shoulder_width\", \"arm_length\", \"leg_length\", \"inseam\", \"neck\"]:\n",
        "            value = input(f\"{key.capitalize()} (cm): \").strip()\n",
        "            ground_truth[key] = float(value) if value else None\n",
        "\n",
        "    test_results = test_measurement_consistency(\n",
        "        front_image_path, side_image_path, height, ground_truth if use_ground_truth else None, num_runs=3\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Measurement Results ===\")\n",
        "    for i, result in enumerate(test_results[\"results\"]):\n",
        "        print(f\"\\nRun {i + 1}:\")\n",
        "        print(\"Measurements (cm):\")\n",
        "        for key, value in result[\"measurements\"].items():\n",
        "            print(f\"  {key.capitalize():<15}: {value:.2f}\")\n",
        "        print(f\"Recommended Size: {result['size']}\")\n",
        "        print(f\"Landmarks Detected: {result['landmark_count']}\")\n",
        "\n",
        "    print(\"\\n=== Consistency Metrics ===\")\n",
        "    print(\"Mean Measurements (cm):\")\n",
        "    for key, mean in test_results[\"means\"].items():\n",
        "        print(f\"  {key.capitalize():<15}: {mean:.2f}\")\n",
        "    print(\"\\nMeasurement Variances (cm²):\")\n",
        "    for key, variance in test_results[\"variances\"].items():\n",
        "        print(f\"  {key.capitalize():<15}: {variance:.4f}\")\n",
        "    print(f\"Size Consistency       : {'Consistent' if test_results['size_consistency'] else 'Inconsistent'}\")\n",
        "    print(f\"Average Processing Time: {test_results['average_processing_time']:.2f} seconds\")\n",
        "    print(f\"Average Landmarks      : {test_results['average_landmark_count']:.1f}\")\n",
        "\n",
        "    if test_results[\"accuracy_metrics\"]:\n",
        "        print(\"\\n=== Accuracy Metrics (Ground Truth) ===\")\n",
        "        for run, metrics in test_results[\"accuracy_metrics\"].items():\n",
        "            print(f\"\\n{run.replace('_', ' ').title()}:\")\n",
        "            for key, value in metrics.items():\n",
        "                if key.endswith(\"_error\"):\n",
        "                    print(f\"  {key.replace('_error', '').capitalize():<15} Error: {value:.2f} cm\")\n",
        "                elif key.endswith(\"_percentage_error\"):\n",
        "                    print(f\"  {key.replace('_percentage_error', '').capitalize():<15} % Error: {value:.2f}%\")\n",
        "                elif key == \"overall_accuracy\":\n",
        "                    print(f\"  Overall Accuracy: {value:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z7YwHnTA-d22",
        "outputId": "e31047ac-0abf-4e90-a87b-e6530f874df6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload front image (full-body, clear, JPEG/PNG):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e921e95d-dbd2-442e-83a6-037d148dcb6f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e921e95d-dbd2-442e-83a6-037d148dcb6f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test0.png to test0.png\n",
            "Upload side image (full-body, clear, JPEG/PNG):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0485431c-6824-465a-a5eb-161615ff7ce4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0485431c-6824-465a-a5eb-161615ff7ce4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-15T10:32:05.173036+0000 INFO Front image: test0.png, Side image: test1.png\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving test1.png to test1.png\n",
            "Enter height in meters (e.g., 1.7 for 170 cm, press Enter for default): 1.7\n",
            "Do you want to provide ground truth measurements? (y/n): n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/intel-isl/MiDaS/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Downloading: \"https://github.com/isl-org/MiDaS/releases/download/v3/dpt_large_384.pt\" to /root/.cache/torch/hub/checkpoints/dpt_large_384.pt\n",
            "100%|██████████| 1.28G/1.28G [00:12<00:00, 112MB/s]\n",
            "2025-04-15T10:32:37.281636+0000 ERROR Failed to initialize DPT model: Ran out of input\n",
            "2025-04-15T10:32:37.327865+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:37.328959+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:37.329730+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:37.441059+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:37.533667+0000 WARNING DPT model not initialized, skipping depth estimation\n",
            "2025-04-15T10:32:37.535036+0000 WARNING DPT model not initialized, skipping depth estimation\n",
            "2025-04-15T10:32:37.536209+0000 INFO Calibrated using nose to ankle: 1.7m, pixel height: 805.6317138671875px\n",
            "2025-04-15T10:32:37.537534+0000 INFO Calibrated pixel-to-meter ratio: 0.0070, Estimated height: 1.70m\n",
            "2025-04-15T10:32:37.573297+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:37.688891+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:37.779710+0000 INFO Run 1: Size = L, Time = 0.27s, Landmarks = 66\n",
            "2025-04-15T10:32:37.827075+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:37.829875+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:37.832054+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:37.924940+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:38.018699+0000 WARNING DPT model not initialized, skipping depth estimation\n",
            "2025-04-15T10:32:38.021499+0000 WARNING DPT model not initialized, skipping depth estimation\n",
            "2025-04-15T10:32:38.024034+0000 INFO Calibrated using nose to ankle: 1.7m, pixel height: 805.6317138671875px\n",
            "2025-04-15T10:32:38.024929+0000 INFO Calibrated pixel-to-meter ratio: 0.0070, Estimated height: 1.70m\n",
            "2025-04-15T10:32:38.050791+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:38.164453+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:38.269792+0000 INFO Run 2: Size = L, Time = 0.25s, Landmarks = 66\n",
            "2025-04-15T10:32:38.318844+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:38.321441+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:38.322221+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:38.419646+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:38.517558+0000 WARNING DPT model not initialized, skipping depth estimation\n",
            "2025-04-15T10:32:38.520208+0000 WARNING DPT model not initialized, skipping depth estimation\n",
            "2025-04-15T10:32:38.522096+0000 INFO Calibrated using nose to ankle: 1.7m, pixel height: 805.6317138671875px\n",
            "2025-04-15T10:32:38.523101+0000 INFO Calibrated pixel-to-meter ratio: 0.0070, Estimated height: 1.70m\n",
            "2025-04-15T10:32:38.545551+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:38.662252+0000 WARNING Unusual aspect ratio. Ensure full-body capture with proper posture.\n",
            "2025-04-15T10:32:38.750897+0000 INFO Run 3: Size = L, Time = 0.25s, Landmarks = 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Measurement Results ===\n",
            "\n",
            "Run 1:\n",
            "Measurements (cm):\n",
            "  Shoulder_width : 48.00\n",
            "  Chest          : 105.00\n",
            "  Hips           : 100.56\n",
            "  Waist          : 84.92\n",
            "  Arm_length     : 80.00\n",
            "  Leg_length     : 95.00\n",
            "  Inseam         : 80.00\n",
            "  Neck           : 40.00\n",
            "Recommended Size: L\n",
            "Landmarks Detected: 66\n",
            "\n",
            "Run 2:\n",
            "Measurements (cm):\n",
            "  Shoulder_width : 48.00\n",
            "  Chest          : 105.00\n",
            "  Hips           : 100.56\n",
            "  Waist          : 84.92\n",
            "  Arm_length     : 80.00\n",
            "  Leg_length     : 95.00\n",
            "  Inseam         : 80.00\n",
            "  Neck           : 40.00\n",
            "Recommended Size: L\n",
            "Landmarks Detected: 66\n",
            "\n",
            "Run 3:\n",
            "Measurements (cm):\n",
            "  Shoulder_width : 48.00\n",
            "  Chest          : 105.00\n",
            "  Hips           : 100.56\n",
            "  Waist          : 84.92\n",
            "  Arm_length     : 80.00\n",
            "  Leg_length     : 95.00\n",
            "  Inseam         : 80.00\n",
            "  Neck           : 40.00\n",
            "Recommended Size: L\n",
            "Landmarks Detected: 66\n",
            "\n",
            "=== Consistency Metrics ===\n",
            "Mean Measurements (cm):\n",
            "  Shoulder_width : 48.00\n",
            "  Chest          : 105.00\n",
            "  Hips           : 100.56\n",
            "  Waist          : 84.92\n",
            "  Arm_length     : 80.00\n",
            "  Leg_length     : 95.00\n",
            "  Inseam         : 80.00\n",
            "  Neck           : 40.00\n",
            "\n",
            "Measurement Variances (cm²):\n",
            "  Shoulder_width : 0.0000\n",
            "  Chest          : 0.0000\n",
            "  Hips           : 0.0000\n",
            "  Waist          : 0.0000\n",
            "  Arm_length     : 0.0000\n",
            "  Leg_length     : 0.0000\n",
            "  Inseam         : 0.0000\n",
            "  Neck           : 0.0000\n",
            "Size Consistency       : Consistent\n",
            "Average Processing Time: 0.25 seconds\n",
            "Average Landmarks      : 66.0\n"
          ]
        }
      ]
    }
  ]
}